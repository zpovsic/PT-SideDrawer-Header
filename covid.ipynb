{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1_cuszBXXkCpltcO9LNzwfuVdBCMpD9xo",
      "authorship_tag": "ABX9TyP4MrVVxImiAVemH73+/x7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zpovsic/PT-SideDrawer-Header/blob/master/covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c99EvWo1s9-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive') #, force_remount=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SJ-tGNkOeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/Covid-xray-data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OGNpmn43C0O6",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import dill\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pathlib\n",
        "import shutil\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import ceil\n",
        "\n",
        "!pip install -U PyYAML\n",
        "import yaml\n",
        "\n",
        "!pip install -U pillow\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDgMYmwq-oTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.metrics import Metric, Precision, Recall\n",
        "from tensorflow.python.keras.utils import metrics_utils\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.keras.utils.generic_utils import to_list\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, CategoricalAccuracy, Precision, Recall, AUC\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D, Conv2D, Flatten, LeakyReLU, BatchNormalization, \\\n",
        "    concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jKW5byYoSpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV95GNJpEa7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONFIG_PATH = \"/content/drive/My Drive/Covid-xray-data/config.yml\"\n",
        "PATH = \"/content/drive/My Drive/Covid-xray-data/\"\n",
        "DATA_PATH = \"/content/drive/My Drive/Covid-xray-data/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhiwRjH09S4R",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# **Train**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmujXCPNbtIV",
        "colab_type": "text"
      },
      "source": [
        "**Remove text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6yHnQ9qbwe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_text(img):\n",
        "    \"\"\"\n",
        "    Attempts to remove textual artifacts from X-ray images. For example, many images indicate the right side of the\n",
        "    body with a white 'R'. Works only for very bright text.\n",
        "    :param img: Numpy array of image\n",
        "    :return: Array of image with (ideally) any characters removed and inpainted\n",
        "    \"\"\"\n",
        "    mask = cv2.threshold(img, 230, 255, cv2.THRESH_BINARY)[1][:, :, 0].astype(np.uint8)\n",
        "    img = img.astype(np.uint8)\n",
        "    result = cv2.inpaint(img, mask, 10, cv2.INPAINT_NS).astype(np.float32)\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AJOyb_cUca",
        "colab_type": "text"
      },
      "source": [
        "**Get class weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9eIMxgGcZQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_weights(histogram, class_multiplier=None):\n",
        "    \"\"\"\n",
        "    Computes weights for each class to be applied in the loss function during training.\n",
        "    :param histogram: A list depicting the number of each item in different class\n",
        "    :param class_multiplier: List of values to multiply the calculated class weights by. For further control of class weighting.\n",
        "    :return: A dictionary containing weights for each class\n",
        "    \"\"\"\n",
        "    weights = [None] * len(histogram)\n",
        "    for i in range(len(histogram)):\n",
        "        weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]\n",
        "    class_weight = {i: weights[i] for i in range(len(histogram))}\n",
        "    if class_multiplier is not None:\n",
        "        class_weight = [class_weight[i] * class_multiplier[i] for i in range(len(histogram))]\n",
        "    \n",
        "    class_weight_dict = dict(enumerate(class_weight))\n",
        "    print(\"Class weights: \", class_weight, \"Class_weight_dict: \", class_weight_dict)\n",
        "    return class_weight_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krJH2mYhcrLe",
        "colab_type": "text"
      },
      "source": [
        "**Class F1score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nfhka3kcv0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class F1Score(Metric):\n",
        "    \"\"\"\n",
        "    Custom tf.keras metric that calculates the F1 Score\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None):\n",
        "        \"\"\"\n",
        "        Creates an instance of the  F1Score class\n",
        "        :param thresholds: A float value or a python list/tuple of float threshold values in [0, 1].\n",
        "        :param top_k: An int value specifying the top-k predictions to consider when calculating precision\n",
        "        :param class_id: Integer class ID for which we want binary metrics. This must be in the half-open interval\n",
        "                `[0, num_classes)`, where `num_classes` is the last dimension of predictions\n",
        "        :param name: string name of the metric instance\n",
        "        :param dtype: data type of the metric result\n",
        "        \"\"\"\n",
        "        super(F1Score, self).__init__(name=name, dtype=dtype)\n",
        "        self.init_thresholds = thresholds\n",
        "        self.top_k = top_k\n",
        "        self.class_id = class_id\n",
        "\n",
        "        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
        "        self.thresholds = metrics_utils.parse_init_thresholds(\n",
        "            thresholds, default_threshold=default_threshold)\n",
        "        self.true_positives = self.add_weight('true_positives', shape=(len(self.thresholds),),\n",
        "                                              initializer=init_ops.zeros_initializer)\n",
        "        self.false_positives = self.add_weight('false_positives', shape=(len(self.thresholds),),\n",
        "                                               initializer=init_ops.zeros_initializer)\n",
        "        self.false_negatives = self.add_weight('false_negatives', shape=(len(self.thresholds),),\n",
        "                                               initializer=init_ops.zeros_initializer)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Accumulates true positive, false positive and false negative statistics.\n",
        "        :param y_true: The ground truth values, with the same dimensions as `y_pred`. Will be cast to `bool`\n",
        "        :param y_pred: The predicted values. Each element must be in the range `[0, 1]`\n",
        "        :param sample_weight: Weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0,\n",
        "               or the same rank as `y_true`, and must be broadcastable to `y_true`\n",
        "        :return: Update operation\n",
        "        \"\"\"\n",
        "        metrics_utils.update_confusion_matrix_variables(\n",
        "            {\n",
        "                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
        "                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n",
        "                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\n",
        "            },\n",
        "            y_true, y_pred, thresholds=self.thresholds, top_k=self.top_k, class_id=self.class_id,\n",
        "            sample_weight=sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        \"\"\"\n",
        "        Compute the value for the F1 score. Calculates precision and recall, then F1 score.\n",
        "        F1 = 2 * precision * recall / (precision + recall)\n",
        "        :return: F1 score\n",
        "        \"\"\"\n",
        "        precision = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
        "        recall = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
        "        result = math_ops.div_no_nan(2 * precision * recall, precision + recall)\n",
        "        return result[0] if len(self.thresholds) == 1 else result\n",
        "\n",
        "    def reset_states(self):\n",
        "        \"\"\"\n",
        "        Resets all of the metric state variables. Called between epochs, when a metric is evaluated during training.\n",
        "        \"\"\"\n",
        "        num_thresholds = len(to_list(self.thresholds))\n",
        "        K.batch_set_value(\n",
        "            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the serializable config of the metric.\n",
        "        :return: serializable config of the metric\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'thresholds': self.init_thresholds,\n",
        "            'top_k': self.top_k,\n",
        "            'class_id': self.class_id\n",
        "        }\n",
        "        base_config = super(F1Score, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdw6niJOJK4-",
        "colab_type": "text"
      },
      "source": [
        "DCCN resnet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SlVtVG_JRrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dcnn_resnet(model_config, input_shape, metrics, n_classes=2, output_bias=None):\n",
        "    \"\"\"\n",
        "    Defines a deep convolutional neural network model for multiclass X-ray classification.\n",
        "    :param model_config: A dictionary of parameters associated with the model architecture\n",
        "    :param input_shape: The shape of the model input\n",
        "    :param metrics: Metrics to track model's performance\n",
        "    :param n_classes: Number of classes\n",
        "    :param output_bias: Output bias\n",
        "    :return: a Keras Sequential model object with the architecture defined in this method\n",
        "    \"\"\"\n",
        "\n",
        "    # Set hyperparameters\n",
        "    nodes_dense0 = model_config['NODES_DENSE0']\n",
        "    lr = model_config['LR']\n",
        "    dropout = model_config['DROPOUT']\n",
        "    l2_lambda = model_config['L2_LAMBDA']\n",
        "    if model_config['OPTIMIZER'] == 'adam':\n",
        "        optimizer = Adam(learning_rate=lr)\n",
        "    elif model_config['OPTIMIZER'] == 'sgd':\n",
        "        optimizer = SGD(learning_rate=lr)\n",
        "    else:\n",
        "        optimizer = Adam(learning_rate=lr)  # For now, Adam is default option\n",
        "    init_filters = model_config['INIT_FILTERS']\n",
        "    filter_exp_base = model_config['FILTER_EXP_BASE']\n",
        "    conv_blocks = model_config['CONV_BLOCKS']\n",
        "    kernel_size = eval(model_config['KERNEL_SIZE'])\n",
        "    max_pool_size = eval(model_config['MAXPOOL_SIZE'])\n",
        "    strides = eval(model_config['STRIDES'])\n",
        "\n",
        "    # Set output bias\n",
        "    if output_bias is not None:\n",
        "        output_bias = Constant(output_bias)\n",
        "    print(\"MODEL CONFIG: \", model_config)\n",
        "\n",
        "    # Input layer\n",
        "    X_input = Input(input_shape)\n",
        "    X = X_input\n",
        "\n",
        "    # Add convolutional (residual) blocks\n",
        "    for i in range(conv_blocks):\n",
        "        X_res = X\n",
        "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
        "                   kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
        "        X = BatchNormalization()(X)\n",
        "        X = LeakyReLU()(X)\n",
        "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
        "                   kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
        "        X = concatenate([X, X_res])\n",
        "        X = BatchNormalization()(X)\n",
        "        X = LeakyReLU()(X)\n",
        "        X = MaxPool2D(max_pool_size, padding='same')(X)\n",
        "\n",
        "    # Add fully connected layers\n",
        "    X = Flatten()(X)\n",
        "    X = Dropout(dropout)(X)\n",
        "    X = Dense(nodes_dense0, kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    Y = Dense(n_classes, activation='softmax', bias_initializer=output_bias, name='output')(X)\n",
        "\n",
        "    # Set model loss function, optimizer, metrics.\n",
        "    model = Model(inputs=X_input, outputs=Y)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktLURy-UV_z_",
        "colab_type": "text"
      },
      "source": [
        "**Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa3gBOjZV6M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(config, data, callbacks, verbose=1):\n",
        "    \"\"\"\n",
        "    Train a and evaluate model on given data.\n",
        "    :param config: Project config (from config.yml)\n",
        "    :param data: dict of partitioned dataset\n",
        "    :param callbacks: list of callbacks for Keras model\n",
        "    :param verbose: Verbosity mode to pass to model.fit()\n",
        "    :return: Trained model and associated performance metrics on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # If set in config file, oversample the minority class\n",
        "    if config['TRAIN']['IMB_STRATEGY'] == 'random_oversample':\n",
        "        data['TRAIN'] = random_minority_oversample(data['TRAIN'])\n",
        "\n",
        "    # Create ImageDataGenerators\n",
        "    train_img_gen = ImageDataGenerator(rotation_range=10, preprocessing_function=remove_text,\n",
        "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
        "    val_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
        "                                     samplewise_std_normalization=True, samplewise_center=True)\n",
        "    test_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
        "                                      samplewise_std_normalization=True, samplewise_center=True)\n",
        "\n",
        "    # Create DataFrameIterators\n",
        "    img_shape = tuple(config['DATA']['IMG_DIM'])\n",
        "    x_col = 'filename'\n",
        "    y_col = 'label_str'\n",
        "    class_mode = 'categorical'\n",
        "    train_generator = train_img_gen.flow_from_dataframe(dataframe=data['TRAIN'], directory=PATH + \"data/train/\",\n",
        "                                                        x_col=x_col, y_col=y_col, has_ext=True, target_size=img_shape,\n",
        "                                                        batch_size=config['TRAIN']['BATCH_SIZE'], class_mode=class_mode)\n",
        "    val_generator = val_img_gen.flow_from_dataframe(dataframe=data['VAL'], directory=PATH + \"data/val/\",\n",
        "                                                    x_col=\"filename\", y_col=y_col, target_size=img_shape,\n",
        "                                                    batch_size=config['TRAIN']['BATCH_SIZE'], class_mode=class_mode)\n",
        "    test_generator = test_img_gen.flow_from_dataframe(dataframe=data['TEST'], directory=PATH + \"data/test/\",\n",
        "                                                      x_col=\"filename\", y_col=y_col, target_size=img_shape,\n",
        "                                                      batch_size=config['TRAIN']['BATCH_SIZE'], class_mode=class_mode,\n",
        "                                                      shuffle=False)\n",
        "\n",
        "    # Save model's ordering of class indices\n",
        "    dill.dump(test_generator.class_indices, open(PATH + 'data/interpretability/OUTPUT_CLASS_INDICES', 'wb'))\n",
        "\n",
        "    # Apply class imbalance strategy. We have many more X-rays negative for COVID-19 than positive.\n",
        "    histogram = np.bincount(np.array(train_generator.labels).astype(int))  # Get class distribution\n",
        "    class_weight = None\n",
        "    if config['TRAIN']['IMB_STRATEGY'] == 'class_weight':\n",
        "        class_multiplier = config['TRAIN']['CLASS_MULTIPLIER']\n",
        "        class_multiplier = [class_multiplier[config['DATA']['CLASSES'].index(c)] for c in test_generator.class_indices]\n",
        "        class_weight = get_class_weights(histogram, class_multiplier)\n",
        "\n",
        "    # Define metrics.\n",
        "    covid_class_idx = test_generator.class_indices['COVID-19']  # Get index of COVID-19 class\n",
        "    thresholds = 1.0 / len(config['DATA']['CLASSES'])  # Binary classification threshold for a class\n",
        "    metrics = ['accuracy', CategoricalAccuracy(name='categorical_accuracy'),\n",
        "               Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
        "               Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
        "               AUC(name='auc'),F1Score(name='f1score', thresholds=thresholds, class_id=covid_class_idx)] # some error try latter!!!!!!!!\n",
        "\n",
        "    # Define the model.\n",
        "    print('Training distribution: ',\n",
        "          ['Class ' + list(test_generator.class_indices.keys())[i] + ': ' + str(histogram[i]) + '. '\n",
        "           for i in range(len(histogram))])\n",
        "    input_shape = config['DATA']['IMG_DIM'] + [3]\n",
        "    if config['TRAIN']['CLASS_MODE'] == 'binary':\n",
        "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
        "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
        "        model = dcnn_resnet(config['NN']['DCNN_BINARY'], input_shape, metrics, 2, output_bias=output_bias)\n",
        "    else:\n",
        "        n_classes = len(config['DATA']['CLASSES'])\n",
        "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
        "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
        "        model = dcnn_resnet(config['NN']['DCNN_MULTICLASS'], input_shape, metrics, n_classes, output_bias=output_bias)\n",
        "\n",
        "    # Train the model.\n",
        "    steps_per_epoch = ceil(train_generator.n / train_generator.batch_size)\n",
        "    val_steps = ceil(val_generator.n / val_generator.batch_size)\n",
        "\n",
        "    print('\\n*** steps = ',steps_per_epoch, \n",
        "          '\\n*** epochs = ', config['TRAIN']['EPOCHS'], \n",
        "          '\\n*** valdata = ', val_generator, \n",
        "          '\\n*** valsteps = ', val_steps, \n",
        "          \"\\n*** callbacks = \", callbacks, \n",
        "          \"\\n*** class_weight = \", class_weight)\n",
        "\n",
        "    history = model.fit(train_generator,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        epochs=config['TRAIN']['EPOCHS'],\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=val_steps,\n",
        "                        callbacks=callbacks,\n",
        "                        verbose=verbose,\n",
        "                        class_weight=class_weight)  \n",
        "    # Run the model on the test set and print the resulting performance metrics.\n",
        "    test_results = model.evaluate(test_generator, verbose=1)\n",
        "    test_metrics = {}\n",
        "    test_summary_str = [['**Metric**', '**Value**']]\n",
        "    for metric, value in zip(model.metrics_names, test_results):\n",
        "        test_metrics[metric] = value\n",
        "        print(metric, ' = ', value)\n",
        "        test_summary_str.append([metric, str(value)])\n",
        "    return model, test_metrics, test_generator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FfW6H7HS8T6",
        "colab_type": "text"
      },
      "source": [
        "**Train experiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH7_M54V-vPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_experiment(experiment='single_train', save_weights=True, write_logs=True):\n",
        "    \"\"\"\n",
        "    Defines and trains model. Prints and logs relevant metrics.\n",
        "    :param experiment: The type of training experiment. Choices are {'single_train'}\n",
        "    :param save_weights: A flag indicating whether to save the model weights\n",
        "    :param write_logs: A flag indicating whether to write TensorBoard logs\n",
        "    :return: A dictionary of metrics on the test set\n",
        "    \"\"\"\n",
        "   # Load project config data\n",
        "    config = yaml.full_load(open(PATH + '/config.yml', 'r'))\n",
        "\n",
        "    # Set logs directory\n",
        "    cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    log_dir = PATH + \"results\\\\logs\\\\training\\\\\" + cur_date if write_logs else None\n",
        "\n",
        "    # Load dataset file paths and labels\n",
        "    data = {'TRAIN': pd.read_csv(PATH + \"data/train_set.csv\"), 'VAL': pd.read_csv(PATH + \"data/val_set.csv\"),\n",
        "            'TEST': pd.read_csv(PATH + \"data/test_set.csv\")}\n",
        "\n",
        "    # Set callbacks.\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=config['TRAIN']['PATIENCE'], mode='min',\n",
        "                                   restore_best_weights=True)\n",
        "    callbacks = [early_stopping]\n",
        "\n",
        "    # Conduct the desired train experiment\n",
        "    if experiment == 'hparam_search':\n",
        "        log_dir = PATH + \"results\\\\logs\\\\hparam_search\\\\\" + cur_date\n",
        "        random_hparam_search(config, data, callbacks, log_dir)\n",
        "    else:\n",
        "        if experiment == 'multi_train':\n",
        "            base_log_dir = PATH + \"results\\\\logs\\\\training\\\\\" if write_logs else None\n",
        "            model, test_metrics, test_generator, cur_date = multi_train(config, data, callbacks, base_log_dir)\n",
        "        else:\n",
        "            if write_logs:\n",
        "                tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "                callbacks.append(tensorboard)\n",
        "            model, test_metrics, test_generator = train_model(config, data, callbacks)\n",
        "            if write_logs:\n",
        "                log_test_results(config, model, test_generator, test_metrics, log_dir)\n",
        "        if save_weights:\n",
        "            model_path = PATH + \"results\\\\logs\\\\models\\\\model\" + cur_date + '.h5'\n",
        "            print(model_path)\n",
        "            save_model(model, model_path)  # Save the model's weights\n",
        "    return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuEgRihL--bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = yaml.full_load(open(CONFIG_PATH, 'r'))\n",
        "\n",
        "train_experiment(experiment=config['TRAIN']['EXPERIMENT_TYPE'], save_weights=True, write_logs=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}